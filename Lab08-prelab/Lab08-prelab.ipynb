{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0607874-fa9a-44d1-a7af-2c5db485891a",
   "metadata": {},
   "source": [
    "# Lab 08 Prelab: Analytic chi-squared minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b70d5-9afc-45b0-bd6a-56ba39311b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_entry\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c7a22-ebd3-4977-9d87-395137731c65",
   "metadata": {},
   "source": [
    "In Lab 08 you will continue to collect data from the RC circuit in an effort to improve your dataset. In addition, this prelab will provide you with an analytic formula for minimizing chi-squared. This formula allows you to calculate the best fitting parameters to your data automatically, instead of iteratively searching for the best fit parameters yourself.\n",
    "\n",
    "However, a reminder that determining the best fit to your data does not allow you to say \"this is a *good* fit to my data\". The best fit, by definition, is one that minimizes chi-squared. But if your minimized chi-squared is $\\chi^2=11.8$, we now know this is not a good fit to your data. Conversely, if your minimized chi-squared is $\\chi^2=0.05$, then it may be a good fit to the data, but you should take a closer look at our uncertainty estimation strategy as our uncertainties are likely overestimated.\n",
    "\n",
    "All of this is to say that just because the chi-squared minimization process can now be automated in the lab, this does not mean you can discard previous tools like residuals plots and the process of interpreting the minimized chi-squared. We have built up fitting to data in a piecemeal fashion for you, from uncertainty estimation, to weighted mean, to scatter plots and residuals, to chi-squared calculation, to iterative and now automated chi-squared minimization. This way, you now have a thorough understanding of what goes into each component of determining the best fit, as well as interpreting the goodness of fit in a systematic way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d44b16-605b-4d2c-b5f6-a5679404bc55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## An analytic formula for minimizing chi-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ee9a5-d5b5-47bf-b100-b396faa1bc8e",
   "metadata": {},
   "source": [
    "Recall that chi-squared is a continuous function of the fitting parameters in your model. Specifically, if you have $P$ parameters in your model, then chi-squared is a $P$-dimensional function. For instance, if we are fitting a one-parameter linear model, $y=mx$, then $m$ is the sole parameter and the associated chi-squared function is $\\chi^2(m)$. For a two-parameter linear model, $y=mx+b$, then we would have $\\chi^2(m,b)$. \n",
    "\n",
    "From your first-year calculus course, you may know that we can take a continuous function $f(x)$ and find the minimum or maximum (a critical point, $x_c$) by taking the derivative of $f(x)$ with respect to $x$, setting this derivative equal to zero, and solving the resulting equation\n",
    "\n",
    "$$\\left[\\frac{df(x)}{dx}\\right]_{x=x_c}=0.$$\n",
    "\n",
    "Since $\\chi^2$ is a continuous function, we can do exactly this process to come up with an expression that automatically calculates the critical point(s), rather than iteratively adjusting the fitting parameter(s) to find the minimum. In other words, we can use calculus to derive an *analytic* expression for the best fit parameter(s).\n",
    "\n",
    "*NOTE! for those thinking: wait a minute, how do we know the critical point will be a minimum and not a maximum? If we think about fitting a model to data that is finite, we know that as we move the parameters to $+\\infty$ and $-\\infty$ the fit will become increasingly worse to the data, meaning the residuals and thus chi-squared will continue to increase. This means that there is no maximum chi-squared, since we can always make the parameters worse by continuing to move them in the \"wrong\" direction, i.e., away from the best fit value. From this observation is follows that any critical point found must be a minimum.*\n",
    "\n",
    "In the simplest case of a one-parameter linear model, $y=mx$, we wish to minimize chi-squared with respect to $m$ to find the best fit slope\n",
    "\n",
    "$$ \\frac{d\\chi^2(m)}{dm} = 0.$$\n",
    "\n",
    "We can first substitute our general expression for chi-squared\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - f(x_i)}{u[y_i]}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Our model is $f(x_i) = mx_i$, which we can substitute into the above expression\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{u[y_i]}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Since the derivative is with respect to $m$, it has no effect on $N$ or $P$, meaning we can move the derivative inside the brackets\n",
    "\n",
    "$$ \\frac{1}{N-P} \\frac{d}{dm} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{u[y_i]}\\right)^2 = 0.$$\n",
    "\n",
    "The summation is only over variables with a subscript \"$i$\"; $m$ does not contain this so we can also switch the order of differentiation and summation\n",
    "\n",
    "$$ \\frac{1}{N-P} \\sum_{i=1}^N \\frac{d}{dm} \\left(\\frac{y_i - mx_i}{u[y_i]}\\right)^2 = 0.$$\n",
    "\n",
    "Now we perform some calculus and take the derivative (invoking the chain rule)\n",
    "\n",
    "$$ \\frac{2}{N-P} \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{u[y_i]}\\right) \\cdot \\frac{d}{dm} \\left(\\frac{y_i - mx_i}{u[y_i]}\\right)= 0$$\n",
    "\n",
    "$$ \\frac{2}{N-P} \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{u[y_i]}\\right) \\cdot -\\frac{x_i}{u[y_i]} = 0.$$\n",
    "\n",
    "The negative sign can be taken outside the sum, and since we are setting everything equal to zero the $2/(N-P)$ can be discarded\n",
    "\n",
    "$$ \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{u[y_i]}\\right) \\cdot \\frac{x_i}{u[y_i]} = 0.$$\n",
    "\n",
    "What remains is to rearrange this expression for $m$. We can start by expanding the terms in the summation\n",
    "\n",
    "$$ \\sum_{i=1}^N  \\left(\\frac{y_i}{u[y_i]} - m\\frac{x_i}{u[y_i]}\\right) \\cdot \\frac{x_i}{u[y_i]} = 0$$\n",
    "$$ \\sum_{i=1}^N  \\frac{x_iy_i}{(u[y_i])^2} - m\\frac{x_i^2}{(u[y_i])^2} = 0$$\n",
    "\n",
    "then finally isolate $m$\n",
    "\n",
    "$$ m = \\frac{\\sum_{i=1}^N  \\frac{x_iy_i}{(u[y_i])^2}}{\\sum_{i=1}^N \\frac{x_i^2}{(u[y_i])^2}} $$\n",
    "\n",
    "So given our $x$ and $y$ data plus the uncertainty in $y$, we are able to analytically solve for the best fit slope using this equation! Since the data have some uncertainty, it seems reasonable that there should be some uncertainty in the best fit slope as well. This can be calculated by performing uncertainty propagation on the above expression. A derivation of this is beyond the scope of this prelab, so we provide the answer for you below:\n",
    "\n",
    "$$ u[m] = \\sqrt{\\frac{1}{\\sum_{i=1}^N \\frac{x_i^2}{(u[y_i])^2}}} .$$\n",
    "\n",
    "Since the term $\\sum_{i=1}^N \\frac{x_i^2}{(u[y_i])^2}$ appears both in $m$ and $u[m]$, it is convenient for notation and coding purposes to define a placeholder variable\n",
    "\n",
    "$$ Z = \\sum_{i=1}^N \\frac{x_i^2}{(u[y_i])^2} $$\n",
    "\n",
    "such that\n",
    "\n",
    "$$ m = \\frac{1}{Z} \\sum_{i=1}^N  \\frac{x_iy_i}{(u[y_i])^2} $$\n",
    "$$ u[m] = \\sqrt{\\frac{1}{Z}} .$$\n",
    "\n",
    "\n",
    "***Question:*** Take a close look at the expression for $u[m]$. In an experiment where you are trying to model the behaviour of a system as well as possible, an overarching goal is to make the relative uncertainty of your fitting parameters as small as possible. How do the number of data points, $N$, and uncertainty in the data, $u[y_i]$, affect relative uncertainty in $m$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad46f59-2db0-4447-acea-cd032c4b3553",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17e81fb5-c900-4b8d-ae55-3d292ffcc10f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Applying the analytic equation to sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc96ecf-5204-4bc9-b413-62a88b0db571",
   "metadata": {},
   "source": [
    "The code below does some of the work to set up these equations for $m$ and $u[m]$ for the sample data of last week's pre-lab. \n",
    "Walk through the code and complete the missing steps, then run the cell to see:\n",
    "\n",
    "- the fits for the best fit slope $m$, along with lines for those at the 68% CI limits: ($m+u[m]$), and ($m-u[m]$), all plotted with the data; and\n",
    "- the corresponding residuals plots;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c920f5-c74b-48e5-960c-236a4ddcdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sample data \"\"\"\n",
    "xdata = np.array([0.1, 0.16428571, 0.22857143, 0.29285714, 0.35714286, 0.42142857, 0.48571429, 0.55, 0.61428571, 0.67857143, \\\n",
    "                   0.74285714, 0.80714286, 0.87142857, 0.93571429, 1.])\n",
    "ydata = np.array([0.33336864, 0.5414786, 0.82003978, 1.09858314, 1.27560974, 1.52025082, 1.67681586, 2.03833678, \\\n",
    "                  2.35943739, 2.36120224, 2.74941308, 2.83963194, 2.9932707, 3.40978616, 3.44578725])\n",
    "uydata = np.array([0.01666843, 0.02707393, 0.04100199, 0.05492916, 0.06378049, 0.07601254, 0.08384079, 0.10191684, \\\n",
    "                   0.11797187, 0.11806011, 0.13747065, 0.1419816, 0.14966353, 0.17048931, 0.17228936])\n",
    "\n",
    "\"\"\" constructing model, calculating residuals and chi-squared \"\"\"\n",
    "\n",
    "# calculations for the analytic best fit here: \n",
    "Z = np.sum((xdata * xdata) / uydata**2) # calculating sum(x_i*x_i/(u[y_i])^2)\n",
    "\n",
    "# FILL IN THE MISSING STEPS:\n",
    "# m = # calculating best fit slope\n",
    "# um = # calculating uncertainty in best fit slope\n",
    "print(\"Best fit slope m = {:.3f} +/- {:.3f}\".format(m, um))\n",
    "\n",
    "mMax = m + um # maximum reasonable slope\n",
    "mMin = m - um # minimum reasonable slope\n",
    "\n",
    "ymodelBest = m * xdata # best fit model\n",
    "ymodelMax = mMax * xdata # max model\n",
    "ymodelMin = mMin * xdata # min model\n",
    "\n",
    "res = ydata - ymodelBest # calculating residuals (best fit)\n",
    "wres2 = (res/uydata)**2 # weighted residuals squared\n",
    "    \n",
    "N = len(xdata) # number of data points\n",
    "P = 1 # number of parameters\n",
    "chi2 = np.sum(wres2) / (N - P) # calculating chi-squared\n",
    "print(\"chi2 = {:.4f}\".format(chi2))\n",
    "\n",
    "\"\"\" plotting data and fits \"\"\"\n",
    "\n",
    "plt.errorbar(xdata, ydata, uydata, marker='.', linestyle='', color='k')\n",
    "plt.plot(xdata, ymodelBest, label=\"best fit\")\n",
    "plt.plot(xdata, ymodelMax, label=\"max fit\")\n",
    "plt.plot(xdata, ymodelMin, label=\"min fit\")\n",
    "plt.xlabel('x data')\n",
    "plt.ylabel('y data')\n",
    "plt.title('Data with fit')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" plotting residuals (best fit) \"\"\"\n",
    "\n",
    "plt.errorbar(xdata, res, uydata, marker='.', linestyle='')\n",
    "plt.xlabel('x data')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals plot (best fit, $\\chi^2$={:.4f})'.format(chi2))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" plotting residuals (max  fit) \"\"\"\n",
    "\n",
    "resMax = ydata - ymodelMax # calculating residuals (max fit)\n",
    "wres2Max = (resMax/uydata)**2 # weighted residuals squared\n",
    "chi2Max = np.sum(wres2Max) / (N - P) # calculating chi-squared\n",
    "\n",
    "plt.errorbar(xdata, resMax, uydata, marker='.', linestyle='')\n",
    "plt.xlabel('x data')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals plot (max fit, $\\chi^2$={:.4f})'.format(chi2Max))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" plotting residuals (min fit) \"\"\"\n",
    "\n",
    "resMin = ydata - ymodelMin # calculating residuals (min fit)\n",
    "wres2Min = (resMin/uydata)**2 # weighted residuals squared\n",
    "chi2Min = np.sum(wres2Min) / (N - P) # calculating chi-squared\n",
    "\n",
    "plt.errorbar(xdata, resMin, uydata, marker='.', linestyle='')\n",
    "plt.xlabel('x data')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals plot (min fit, $\\chi^2$={:.4f})'.format(chi2Min))\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076950fe-1591-441e-9a0c-23e8e735ad36",
   "metadata": {},
   "source": [
    "You should find the best fit slope is 3.545 +/- 0.046\n",
    "\n",
    "Below, you can see a visualization of how the analytically-determined $m$ lies at the lowest point of the chi-squared vs. $m$ curve (as expected for a minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d8540-6a0a-469d-8f0b-a1c3462aa24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" plotting chi-squared as a function of m \"\"\"\n",
    "\n",
    "mVec = np.linspace(mMin,mMax,100)\n",
    "chi2Vec = np.zeros(np.size(mVec))\n",
    "for i in range(len(mVec)):\n",
    "    ymodelTemp = mVec[i]*xdata # model for the current value of m in the vector\n",
    "    resTemp = ydata - ymodelTemp # residuals for this model\n",
    "    wres2Temp = (resTemp / uydata)**2 # weighting these residuals\n",
    "    chi2Vec[i] = np.sum(wres2Temp) / (N - P) # calculating chi2 from these weighted residuals\n",
    "\n",
    "plt.plot(mVec, chi2Vec, 'k')\n",
    "plt.plot(m, chi2, 'o', label='best fit')\n",
    "plt.plot(mMax, chi2Max, 'o', label='max fit')\n",
    "plt.plot(mMin, chi2Min, 'o', label='min fit')\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('Chi-squared')\n",
    "plt.title('Visualizing chi-squared minimization in chi-squared space')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5112c498-7487-440d-bb39-3cf8e1ebde89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
