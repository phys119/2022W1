{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0607874-fa9a-44d1-a7af-2c5db485891a",
   "metadata": {},
   "source": [
    "# Lab 08 Prelab: Analytic chi-squared minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570b70d5-9afc-45b0-bd6a-56ba39311b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_entry\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c7a22-ebd3-4977-9d87-395137731c65",
   "metadata": {},
   "source": [
    "In Lab 08 you will continue to collect data from the RC circuit in an effort to improve your dataset. In addition, this prelab will provide you with an analytic formula for minimizing chi-squared. This formula allows you to **calculate** the best fitting parameters to your data, instead of iteratively searching for the best fit parameters yourself.\n",
    "\n",
    "However, a reminder that determining the best fit to your data does not allow you to say \"this is a *good* fit to my data\". The best fit, by definition, is one that minimizes chi-squared. But if your minimized chi-squared is $\\chi^2=11.8$, we now know this is not a good fit to your data. Conversely, if your minimized chi-squared is $\\chi^2=0.05$, then it may be a good fit to the data, but you should take a closer look at our uncertainty estimation strategy as our uncertainties are likely overestimated.\n",
    "\n",
    "All of this is to say that just because the chi-squared minimization process can now be automated in the lab, this does not mean you can discard previous tools like residuals plots and the process of interpreting the minimized chi-squared. We have built up fitting to data in a gradual fashion for you, from uncertainty estimation, to weighted mean, to scatter plots and residuals, to chi-squared calculation, to iterative and now automated chi-squared minimization. This way, you now have a thorough understanding of what goes into each component of determining the best fit, as well as interpreting the goodness of fit in a systematic way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d44b16-605b-4d2c-b5f6-a5679404bc55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## An analytic formula for minimizing chi-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ee9a5-d5b5-47bf-b100-b396faa1bc8e",
   "metadata": {},
   "source": [
    "Recall that chi-squared is a continuous function of the fitting parameters in your model. Specifically, if you have $P$ parameters in your model, then chi-squared is a $P$-dimensional function. For instance, if we are fitting a one-parameter linear model, $y=mx$, then $m$ is the sole parameter and the associated chi-squared function is $\\chi^2(m)$. For a two-parameter linear model, $y=mx+b$, then we would have $\\chi^2(m,b)$. \n",
    "\n",
    "For simple model functions like straight lines, we can use calculus to analytically find the best values of the parameters without having to iteratively search for them. Below, we'll work through that calculation for the case of a straight line fit with no intercept. It is not crucial that you understand all the steps here.\n",
    "\n",
    "From your first-year calculus course, you may know that we can take a continuous function $f(x)$ and find the minimum or maximum (a critical point, $x_c$) by taking the derivative of $f(x)$ with respect to $x$, setting this derivative equal to zero, and solving the resulting equation\n",
    "\n",
    "$$\\left[\\frac{df(x)}{dx}\\right]_{x=x_c}=0.$$\n",
    "\n",
    "Since $\\chi^2$ is a continuous function, we can do exactly this process to come up with an expression that automatically calculates the critical point(s), to find the minimum. In other words, we can use calculus to derive an *analytic* expression for the best fit parameter(s).\n",
    "\n",
    "*NOTE! for those thinking: wait a minute, how do we know the critical point will be a minimum and not a maximum? If we think about fitting a model to data that is finite, we know that as we move the parameters to $+\\infty$ and $-\\infty$ the fit will become increasingly worse to the data, meaning the residuals and thus chi-squared will continue to increase. This means that there is no maximum chi-squared, since we can always make the parameters worse by continuing to move them in the \"wrong\" direction, i.e., away from the best fit value. From this observation is follows that any critical point found must be a minimum.*\n",
    "\n",
    "In the simplest case of a one-parameter linear model, $y=mx$, we wish to minimize chi-squared with respect to $m$ to find the best fit slope\n",
    "\n",
    "$$ \\frac{d\\chi^2(m)}{dm} = 0.$$\n",
    "\n",
    "We can first substitute our general expression for chi-squared\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - f(x_i)}{u[y_i]}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Our model is $f(x_i) = mx_i$, which we can substitute into the above expression\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{u[y_i]}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Since the derivative is with respect to $m$, it has no effect on $N$ or $P$, meaning we can move the derivative inside the brackets\n",
    "\n",
    "$$ \\frac{1}{N-P} \\frac{d}{dm} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{u[y_i]}\\right)^2 = 0.$$\n",
    "\n",
    "The summation is only over variables with a subscript \"$i$\"; $m$ does not contain this so we can also switch the order of differentiation and summation\n",
    "\n",
    "$$ \\frac{1}{N-P} \\sum_{i=1}^N \\frac{d}{dm} \\left(\\frac{y_i - mx_i}{u[y_i]}\\right)^2 = 0.$$\n",
    "\n",
    "Now we perform some calculus and take the derivative (invoking the chain rule)\n",
    "\n",
    "$$ \\frac{2}{N-P} \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{u[y_i]}\\right) \\cdot \\frac{d}{dm} \\left(\\frac{y_i - mx_i}{u[y_i]}\\right)= 0$$\n",
    "\n",
    "$$ \\frac{2}{N-P} \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{u[y_i]}\\right) \\cdot -\\frac{x_i}{u[y_i]} = 0.$$\n",
    "\n",
    "The negative sign can be taken outside the sum, and since we are setting everything equal to zero the $2/(N-P)$ can be discarded\n",
    "\n",
    "$$ \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{u[y_i]}\\right) \\cdot \\frac{x_i}{u[y_i]} = 0.$$\n",
    "\n",
    "What remains is to rearrange this expression for $m$. We can start by expanding the terms in the summation\n",
    "\n",
    "$$ \\sum_{i=1}^N  \\left(\\frac{y_i}{u[y_i]} - m\\frac{x_i}{u[y_i]}\\right) \\cdot \\frac{x_i}{u[y_i]} = 0$$\n",
    "$$ \\sum_{i=1}^N  \\frac{x_iy_i}{(u[y_i])^2} - m\\frac{x_i^2}{(u[y_i])^2} = 0$$\n",
    "\n",
    "then finally isolate $m$\n",
    "\n",
    "$$ m = \\frac{\\sum_{i=1}^N  \\frac{x_iy_i}{(u[y_i])^2}}{\\sum_{i=1}^N \\frac{x_i^2}{(u[y_i])^2}} $$\n",
    "\n",
    "So given our $x$ and $y$ data plus the uncertainty in $y$, we are able to analytically solve for the best fit slope using this equation! The uncertainty in this slope can be determined from the uncertainties in the data by uncertainty propagation. The result is:\n",
    "\n",
    "$$ u[m] = \\sqrt{\\frac{1}{\\sum_{i=1}^N \\frac{x_i^2}{(u[y_i])^2}}} .$$\n",
    "\n",
    "Since the term $\\sum_{i=1}^N \\frac{x_i^2}{(u[y_i])^2}$ appears both in $m$ and $u[m]$, it is convenient for notation and coding purposes to define a placeholder variable\n",
    "\n",
    "$$ Z = \\sum_{i=1}^N \\frac{x_i^2}{(u[y_i])^2} $$\n",
    "\n",
    "such that\n",
    "\n",
    "$$ m = \\frac{1}{Z} \\sum_{i=1}^N  \\frac{x_iy_i}{(u[y_i])^2} $$\n",
    "$$ u[m] = \\sqrt{\\frac{1}{Z}} .$$\n",
    "\n",
    "\n",
    "***Question:*** Take a close look at the expression for $u[m]$. In an experiment where you are trying to model the behaviour of a system as well as possible, an overarching goal is to make the relative uncertainty of your fitting parameters as small as possible. How do the number of data points, $N$, and uncertainies in the data, $u[y_i]$, affect relative uncertainty in $m$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad46f59-2db0-4447-acea-cd032c4b3553",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17e81fb5-c900-4b8d-ae55-3d292ffcc10f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Applying the analytic equation to sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc96ecf-5204-4bc9-b413-62a88b0db571",
   "metadata": {},
   "source": [
    "The code below does some of the work to set up these equations for $m$ and $u[m]$ for the sample data of last week's pre-lab. \n",
    "Walk through the code and complete the missing steps, then run the cell to see:\n",
    "\n",
    "- the fits for the best fit slope $m$, along with lines for those at the 68% CI limits: ($m+u[m]$), and ($m-u[m]$), all plotted with the data; and\n",
    "- the corresponding residuals plots;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c920f5-c74b-48e5-960c-236a4ddcdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sample data \"\"\"\n",
    "xdata = np.array([0.1, 0.16428571, 0.22857143, 0.29285714, 0.35714286, 0.42142857, 0.48571429, 0.55, 0.61428571, 0.67857143, \\\n",
    "                   0.74285714, 0.80714286, 0.87142857, 0.93571429, 1.])\n",
    "ydata = np.array([0.33336864, 0.5414786, 0.82003978, 1.09858314, 1.27560974, 1.52025082, 1.67681586, 2.03833678, \\\n",
    "                  2.35943739, 2.36120224, 2.74941308, 2.83963194, 2.9932707, 3.40978616, 3.44578725])\n",
    "uydata = np.array([0.01666843, 0.02707393, 0.04100199, 0.05492916, 0.06378049, 0.07601254, 0.08384079, 0.10191684, \\\n",
    "                   0.11797187, 0.11806011, 0.13747065, 0.1419816, 0.14966353, 0.17048931, 0.17228936])\n",
    "\n",
    "\"\"\" construct model, calculate residuals and chi-squared \"\"\"\n",
    "\n",
    "# calculations for the analytic best fit here: \n",
    "Z = np.sum((xdata * xdata) / uydata**2) # calculate sum(x_i*x_i/(u[y_i])^2)\n",
    "\n",
    "# FILL IN THE MISSING STEPS:\n",
    "# m = # calculate best fit slope\n",
    "# um = # calculate uncertainty in best fit slope\n",
    "print(\"Best fit slope m = {:.3f} +/- {:.3f}\".format(m, um))\n",
    "\n",
    "mMax = m + um # slope at maximum of 68% CI \n",
    "mMin = m - um # slope at minimum of 68% CI \n",
    "\n",
    "ymodelBest = m * xdata # best fit model\n",
    "ymodelMax = mMax * xdata # max model\n",
    "ymodelMin = mMin * xdata # min model\n",
    "\n",
    "res = ydata - ymodelBest # calculate residuals (best fit)\n",
    "wres2 = (res/uydata)**2 # weighted residuals squared\n",
    "    \n",
    "N = len(xdata) # number of data points\n",
    "P = 1 # number of parameters\n",
    "chi2 = np.sum(wres2) / (N - P) # calculate chi-squared\n",
    "print(\"chi2 = {:.4f}\".format(chi2))\n",
    "\n",
    "\"\"\" plot data and fits \"\"\"\n",
    "\n",
    "plt.errorbar(xdata, ydata, uydata, marker='.', linestyle='', color='k')\n",
    "plt.plot(xdata, ymodelBest, label=\"best fit\")\n",
    "plt.plot(xdata, ymodelMax, label=\"max fit\")\n",
    "plt.plot(xdata, ymodelMin, label=\"min fit\")\n",
    "plt.xlabel('x data')\n",
    "plt.ylabel('y data')\n",
    "plt.title('Data with fit')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" plot residuals (best fit) \"\"\"\n",
    "\n",
    "plt.errorbar(xdata, res, uydata, marker='.', linestyle='')\n",
    "plt.xlabel('x data')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals plot (best fit, $\\chi^2$={:.4f})'.format(chi2))\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076950fe-1591-441e-9a0c-23e8e735ad36",
   "metadata": {},
   "source": [
    "You should find the best fit slope is 3.545 +/- 0.046\n",
    "\n",
    "Below, you can see a visualization of how the analytically-determined ùëö\n",
    "lies at the lowest point of the chi-squared vs. ùëö\n",
    "\n",
    "curve (as expected for a minimum). An array of many different m values is created, and chi-squared is calculated for each of those different slope values. Finally, the calculated chi-squareds are plotted versus slope.\n",
    "\n",
    "The code below uses a programming construction of python called a \"for\" loop. You will not need to know how to use these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d8540-6a0a-469d-8f0b-a1c3462aa24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" plot chi-squared as a function of m \"\"\"\n",
    "\n",
    "mVec = np.linspace(mMin,mMax,100) # prepare an array of different slope values\n",
    "chi2Vec = np.zeros(np.size(mVec)) # create an array of chi-squared values, set each to 0 for now.\n",
    "\n",
    "for i in range(len(mVec)): # loop through all the different m values.\n",
    "    # the indented code below is executed once for each of the m values \n",
    "    # we calculate chi-squared for each possible slope.\n",
    "    ymodelTemp = mVec[i]*xdata # model for the current value of m in the vector\n",
    "    resTemp = ydata - ymodelTemp # residuals for this model\n",
    "    wres2Temp = (resTemp / uydata)**2 # weighting these residuals\n",
    "    chi2Vec[i] = np.sum(wres2Temp) / (N - P) # store chi2 in the i'th position of chi2Vec.\n",
    "    \n",
    "plt.plot(mVec, chi2Vec, 'k')\n",
    "plt.plot(m, chi2, 'o', label='best fit')\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('Chi-squared')\n",
    "plt.title('Visualizing chi-squared minimization in chi-squared space')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a58820-221e-46dd-9675-9052f2e14c0b",
   "metadata": {},
   "source": [
    "## Prepare for Lab 08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cd5f7-2f56-4829-bb14-58187c9acd32",
   "metadata": {},
   "source": [
    "In Lab 08, you will use the analytic formula derived above to calculate the best fit slope for your time-constant versus resistance data. To prepare, you can adapt the calculations above for your Lab 07 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89559f10-928a-4540-8bd8-5ae30f1f0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need to adjust the source filename below to match the name of your Lab 07 data file:\n",
    "de = data_entry.sheet_copy('../Lab07/lab07data2','lab07data2')\n",
    "\n",
    "# If you did the make up lab for lab 7, start with:\n",
    "# de = data_entry.sheet_copy('../Make-ups/Lab07-makeup/lab07data2','lab07-data2')\n",
    "\n",
    "# now calculate Z, m, and u[m]\n",
    "\n",
    "# then calculate chi-squared with calculated m. Is this chi-squared lower than the one you found\n",
    "# by hand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7dd9f-aed0-4dd5-be71-11e9c1a3423b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "193db82c-8f80-4c7b-8c06-3323a4d98aa0",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Please re-run the notebook, check that it executed correctly, then export and upload to Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befbf55b-ac43-4216-9726-54bbb707d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sheets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
